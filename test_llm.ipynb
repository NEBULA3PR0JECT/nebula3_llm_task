{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DBBase' from 'database.arangodb' (/usr/local/lib/python3.9/dist-packages/database/arangodb.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm_orchestration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelineApi, PipelineTask\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sys.path.insert(0, \"/notebooks/nebula3_experiments\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from vg_eval import VGEvaluation, get_sc_graph, spice_get_triplets, tuples_from_sg\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/nebula3_llm_task/llm_orchestration.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTuple\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatabase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marangodb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseConnector, DBBase\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NEBULA_CONF\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisual_clues\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvlm_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VlmFactory\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DBBase' from 'database.arangodb' (/usr/local/lib/python3.9/dist-packages/database/arangodb.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple\n",
    "from llm_orchestration import *\n",
    "from experts.pipeline.api import PipelineApi, PipelineTask\n",
    "# sys.path.insert(0, \"/notebooks/nebula3_experiments\")\n",
    "# from vg_eval import VGEvaluation, get_sc_graph, spice_get_triplets, tuples_from_sg\n",
    "from movie.movie_db import MOVIE_DB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline as transformer_pipeline, set_seed, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from langchain import HuggingFaceHub, OpenAI\n",
    "from langchain.model_laboratory import ModelLaboratory\n",
    "\n",
    "def test_pipeline_task(pipeline_id):\n",
    "    class LlmTask(PipelineTask):\n",
    "        def __init__(self):\n",
    "            self.llm_task = LlmTaskInternal()\n",
    "            print(\"LlmTask Initialized successfully.\")\n",
    "\n",
    "        def process_movie(self, movie_id: str) -> Tuple[bool, str]:\n",
    "            print (f'LlmTask: handling movie: {movie_id}')\n",
    "\n",
    "            output = self.llm_task.process_movie(movie_id)\n",
    "\n",
    "            print(\"LlmTask: Finished handling movie.\")\n",
    "            print(output)\n",
    "            return output\n",
    "        def get_name(self) -> str:\n",
    "            return \"llm\"\n",
    "\n",
    "    pipeline = PipelineApi(None)\n",
    "    task = LlmTask()\n",
    "    pipeline.handle_pipeline_task(task, pipeline_id, stop_on_failure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = PipelineApi(None)\n",
    "\n",
    "class LLMBase(ABC):\n",
    "    @abstractmethod\n",
    "    def completion(prompt_template: str, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "class HuggingFaceLLM(LLMBase):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def completion(self, prompt_template: str, *args, **kwargs):\n",
    "        prompt = prompt_template.format(*args)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(inputs, **kwargs)\n",
    "        return [self.tokenizer.decode(x) for x in outputs]\n",
    "        \n",
    "\n",
    "class OptLLM(LLMBase):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def completion(self, prompt_template: str, *args, **kwargs):\n",
    "        prompt = prompt_template.format(*args)\n",
    "        response = self.model(prompt, max_new_tokens=256, max_length=len(prompt)+256, **kwargs)\n",
    "        return [x['generated_text'].strip() for x in response]        \n",
    "\n",
    "def gpt_execute(prompt_template, *args, **kwargs):            \n",
    "    prompt = prompt_template.format(*args)   \n",
    "    response = openai.Completion.create(prompt=prompt, max_tokens=256, **kwargs)   \n",
    "    # return response\n",
    "    return [x['text'].strip() for x in response['choices']]\n",
    "def opt_execute(prompt_template, *args, **kwargs):            \n",
    "    prompt = prompt_template.format(*args)\n",
    "    response = opt_generator(prompt, max_new_tokens=256, max_length=len(prompt)+256, **kwargs)\n",
    "    print('Prompt length is {}'.format(len(prompt)))\n",
    "    # return [x['generated_text'].strip() for x in response]   \n",
    "    return [x['generated_text'][len(prompt):].strip() for x in response]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebula_db = NEBULA_DB()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = nebula_db.get_llm_key()\n",
    "# nebula_db.change_db(\"nebula_playground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = LlmTaskInternal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = MovieImageId(\"Movies/-3103202934810463453\",90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.nebula_db.get_image_id_from_collection(2369414)\n",
    "print(task.nebula_db.pg_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nebula_db.get_doc_by_key(image_id_as_dict(mid),'s4_visual_clues')\n",
    "nebula_db.get_doc_by_key(image_id_as_dict(mid),'s4_visual_clues')\n",
    "# nebula_db.get_movie_frame_from_collection(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebula_db.write_doc_by_key({'gil': 5, 'dan': 15, 'tali': 20},collection_name='giltest', overwrite = True, key_list=['gil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nebula_db.db.collection('giltest').find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl.get_new_movies(\"2bda2110-bcb8-4a6d-a334-455a1cf30c6c\",\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline_task(\"0cb4accc-14ff-46f7-bbb5-55b085afabeb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = MovieImageId(\"Movies/-6295549713179447550\",0)\n",
    "mobj = task.nebula_db.get_movie_frame_from_collection(mid)\n",
    "mobj['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.prompt_obj.get_prompt(2369414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = task.process_target_id(mid,image_url=mobj['url'],n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.nebula_db.write_movie_frame_doc_to_collection(mid,rc,LLM_OUTPUT_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.process_movie(\"Movies/8477229371170297745\",n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "openai_llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16) # google/flan-t5-xl\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16) \n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# set_seed(14)\n",
    "# ul2_generator = transformer_pipeline('text-generation', model=\"google/ul2\", do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man wiht a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. <extra_id_0>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_length=300)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.random.choice(task.s3_ids,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = task.prompt_obj.generate_prompt(train_ids, mid)\n",
    "print(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc = \"What would an American in France find really weird?\"\n",
    "# input_string = \"[NLG] \"+rc+\" <extra_id_0>\"\n",
    "# input_string = \"[S2S] \" + rc\n",
    "input_string = rc#+\" <extra_id_0>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_length=300, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [HuggingFaceHub(repo_id=\"gpt2\"), OpenAI(temperature=0.2), HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.7}), HuggingFaceHub(repo_id=\"facebook/opt-30b\", model_kwargs={\"temperature\":0.7})]\n",
    "model_lab = ModelLaboratory.from_llms(llms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lab.compare(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(int(time.time()))\n",
    "opt_generator = transformer_pipeline('text-generation', model=\"facebook/opt-2.7b\", do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_execute(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-30b\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/ul2\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16) # google/flan-t5-xl\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-30b\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16) .to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(rc, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc1 = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc1[len(rc):])\n",
    "# print(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_data = json.load(open(IPC_PATH,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ipc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /storage/vg_data/ipc_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = \"/storage/vg_data/ipc_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ipc_images(n=100):\n",
    "    ipc_data = json.load(open(IPC_PATH,'r'))\n",
    "    download_path = \"/storage/vg_data/ipc_images\"\n",
    "    for obj in ipc_data[:n]:\n",
    "        print(\"Downloading \"+obj['url'])\n",
    "        if os.path.exists(os.path.join(download_path, os.path.split(obj['url'])[1])):\n",
    "            print(\"Already exists\")\n",
    "        else:\n",
    "            wget.download(obj['url'],out=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ipc_images(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [len(x['paragraph']) for x in ipc_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = {x['image_id']: x['paragraph'] for x in ipc_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = [len(z1[x]) for x in task.s3_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
